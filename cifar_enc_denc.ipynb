{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    from matplotlib import pyplot as plt\n",
    "    MATPLOTLIB = True\n",
    "except:\n",
    "    MATPLOTLIB = False\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDataSet(Dataset):\n",
    "    def __init__(self, batch_dir_path, train=True):\n",
    "        super().__init__()\n",
    "        if not os.path.isdir(batch_dir_path):\n",
    "            raise FolderDoesNotExist(batch_dir_path)\n",
    "        all_batch_files = sorted(glob(f\"{batch_dir_path}/data_batch_*\"))\n",
    "        if not all_batch_files:\n",
    "            raise FileNotFoundError(f\"No data_batch_n files found in {batch_dir_path}\")\n",
    "\n",
    "        lst_imgs, lst_labels = [], []\n",
    "        if train:\n",
    "            # return the 1st n-1 batches for training\n",
    "            for fi_path in all_batch_files[:-1]:\n",
    "                i, l = self.unpickle(fi_path)\n",
    "                lst_imgs.append(i)\n",
    "                lst_labels.extend(l)\n",
    "            self.data = np.row_stack(lst_imgs).reshape((-1, 3, 32, 32))\n",
    "            self.labels = lst_labels\n",
    "        else:\n",
    "            # return the nth batch for testing\n",
    "            for fi_path in all_batch_files[-1:]:\n",
    "                i, l = self.unpickle(fi_path)\n",
    "                lst_imgs.append(i)\n",
    "                lst_labels.extend(l)\n",
    "            self.data = np.row_stack(lst_imgs).reshape((-1, 3, 32, 32))\n",
    "            self.labels = lst_labels\n",
    "\n",
    "    def unpickle(self, f):\n",
    "        with open(f, \"rb\") as fo:\n",
    "            dct = pickle.load(fo, encoding=\"bytes\")\n",
    "        # normalizing\n",
    "        return dct.get(b\"data\") / 255.0, dct.get(b\"labels\")\n",
    "\n",
    "    def get_image(self, index, plot=False):\n",
    "        i = self.data[index]\n",
    "        l = self.labels[index]\n",
    "        i = np.transpose(i, axes=(1, 2, 0))\n",
    "        if plot and MATPLOTLIB:\n",
    "            plt.imshow(i)\n",
    "        else:\n",
    "            return i\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.data[index].astype(np.float32),\n",
    "            self.labels[index],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = CifarDataSet(\n",
    "            batch_dir_path=\"/home/pranjal/pytorch/Datasets/cifar-10-batches-py/\", train=True\n",
    "        )\n",
    "test = CifarDataSet(\n",
    "    batch_dir_path=\"/home/pranjal/pytorch/Datasets/cifar-10-batches-py/\", train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True),\n",
    "        )\n",
    "encoder_b2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True),\n",
    "        )\n",
    "encoder_b3 = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True),\n",
    "        )\n",
    "encoder_b4 = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True),\n",
    "        )\n",
    "encoder_b5 = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=4, padding=1),\n",
    "        )\n",
    "\n",
    "\n",
    "decoder_b1 = nn.Sequential(\n",
    "        nn.ConvTranspose2d(1024, 512, 4, padding=1),\n",
    "        nn.BatchNorm2d(num_features=512),\n",
    "        )\n",
    "decoder_b2 = nn.MaxUnpool2d(kernel_size=2, stride=2,)\n",
    "decoder_b3 = nn.Sequential(\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(512, 256, 3, padding=1),\n",
    "        nn.BatchNorm2d(num_features=256),\n",
    "        )\n",
    "decoder_b4 = nn.MaxUnpool2d(kernel_size=2, stride=2,)\n",
    "decoder_b5 = nn.Sequential(\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(256, 128, 3, padding=1),\n",
    "        nn.BatchNorm2d(num_features=128),\n",
    "        )\n",
    "decoder_b6 = nn.MaxUnpool2d(kernel_size=2, stride=2,)\n",
    "decoder_b7 = nn.Sequential(\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(128, 64, 3, padding=1),\n",
    "        nn.BatchNorm2d(num_features=64),\n",
    "        )\n",
    "decoder_b8 = nn.MaxUnpool2d(kernel_size=2, stride=2,)\n",
    "decoder_b9 = nn.Sequential(\n",
    "        nn.ReLU(),\n",
    "        nn.ConvTranspose2d(64, 3, 3, padding=1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = DataLoader(train, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat, lab in tl:\n",
    "    op, i1 = encoder_b1(feat)\n",
    "    op, i2 = encoder_b2(op)\n",
    "    op, i3 = encoder_b3(op)\n",
    "    op, i4 = encoder_b4(op)\n",
    "    op = encoder_b5(op)\n",
    "\n",
    "    op = decoder_b1(op)\n",
    "    op = decoder_b2(op, i4)\n",
    "    op = decoder_b3(op)\n",
    "    op = decoder_b4(op, i3)\n",
    "    op = decoder_b5(op)\n",
    "    op = decoder_b6(op, i2)\n",
    "    op = decoder_b7(op)\n",
    "    op = decoder_b8(op, i1)\n",
    "    op = decoder_b9(op)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('gcn': conda)",
   "language": "python",
   "name": "python38564bitgcncondacb855bff8b454c0bbae31f8183fac245"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
